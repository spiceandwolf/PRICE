{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimated cardinality on each single table from the traditional CardEst methods(psql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the cardinality estimated by traditional methods\n",
    "import sqlglot\n",
    "from pilotscope.DBInteractor.PilotDataInteractor import PilotDataInteractor\n",
    "from pilotscope.PilotConfig import PostgreSQLConfig\n",
    "\n",
    "def get_pg_est_card(datasets_name = []):\n",
    "    '''\n",
    "    using pilotscope to get single table ce.\n",
    "    :param datasets_name: the list of names about the datasets to be preprocess\n",
    "    '''\n",
    "    config = PostgreSQLConfig()\n",
    "    config.db_host = \"localhost\"\n",
    "    config.db_user = \"postgres\"\n",
    "    config.db_user_pwd = \"postgres\"\n",
    "    config.db_port = 54323\n",
    "\n",
    "    for dataset_name in datasets_name:\n",
    "        workload_in_path = f'/opt/hdd/datasets/user/cardbench/binary_join_workload/{dataset_name}_without_pg_est_card.sql'\n",
    "        workload_out_path = f'/opt/hdd/datasets/user/cardbench/binary_join_workload/{dataset_name}.sql'\n",
    "        print(f\"dataset: {dataset_name}, workload_in_path: {workload_in_path}, workload_out_path: {workload_out_path}\")\n",
    "        config.db = dataset_name\n",
    "        data_interactor = PilotDataInteractor(config)\n",
    "        \n",
    "        with open(workload_in_path, 'r') as read_files:\n",
    "            sqls = []\n",
    "            for line in read_files:\n",
    "                sql = line.strip()\n",
    "                sqls.append(sql)\n",
    "\n",
    "        subsqls, count = [], 0\n",
    "        for line in sqls:\n",
    "            spilt_infos = line.split(\"||\")\n",
    "            sql, true_card, _ = spilt_infos[0], spilt_infos[1], spilt_infos[2]\n",
    "            data_interactor.pull_subquery_card()\n",
    "            result = data_interactor.execute(sql)\n",
    "            subquerys = list(result.subquery_2_card.keys())\n",
    "            subsqls.append(sql)\n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                print(f\"{count} sqls has been processed\")\n",
    "            for subquery in subquerys:\n",
    "                try:\n",
    "                    tables = [table for table in sqlglot.parse_one(subquery).find_all(sqlglot.exp.Table)]\n",
    "                except Exception as e:\n",
    "                    print(f\"sql: {sql}\")\n",
    "                    print(f\"subquery: {subquery}\")\n",
    "                    break\n",
    "                if len(tables) == 1: # skip single table estimated cardinality\n",
    "                    continue\n",
    "                subsqls.append(f'{subquery};||{true_card}||{result.subquery_2_card[subquery]}')\n",
    "\n",
    "        with open(workload_out_path, \"w\") as f:\n",
    "            for sql in subsqls:\n",
    "                f.write(sql + \"\\n\")\n",
    "\n",
    "        print(f\"dataset: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect suitable sql from cardbench\n",
    "import glob\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from sparse_deferred.structs import graph_struct\n",
    "import sparse_deferred.np as sdnp\n",
    "\n",
    "GraphStruct = graph_struct.GraphStruct\n",
    "InMemoryDB = graph_struct.InMemoryDB\n",
    "\n",
    "def clean_query_string(query):\n",
    "  return query.replace(\"'\", \"'\").replace(\"\\n\", \"\").replace(\";\", \"\").replace(\"bq-cost-models-exp.\", \"\")\n",
    "\n",
    "def get_cardbench_dataset(join_type, dataset_name):\n",
    "    '''\n",
    "    The training datasets are stored sharded (namely split into multiple files)\n",
    "    to find all the shards of a training dataset we use glob. \n",
    "    '''\n",
    "\n",
    "    filename = f\"../CardBench/CardBench_zero_shot_cardinality_training/training_datasets/{join_type}/{dataset_name}_{join_type}.npz\"\n",
    "    filenames = glob.glob(filename + '-*')\n",
    "    filenames.sort(key=lambda f: int(f.split('-')[-1]))\n",
    "    cardinalities = []\n",
    "    queries = []\n",
    "    \n",
    "    for file in tqdm.tqdm(filenames): \n",
    "        np_data = np.load(open(file, 'rb'), allow_pickle=True)  \n",
    "        for k, np_arr in np_data.items():\n",
    "            k_parts = k.split('.')\n",
    "            if k_parts[0] == 'feat' and k_parts[1] == 'n' and k_parts[2] == 'g': \n",
    "                if k_parts[3] == 'cardinality':\n",
    "                    cardinalities += np_arr.tolist()\n",
    "                elif k_parts[3] == 'query':\n",
    "                    queries += np_arr.tolist()\n",
    "    \n",
    "    assert len(cardinalities) == len(queries)\n",
    "    sqls = []\n",
    "    for query, card in zip(queries, cardinalities):\n",
    "        if 'IS NOT NULL' in query.decode():\n",
    "            continue\n",
    "        sqls.append(f'{clean_query_string(query.decode())};||{card}||')\n",
    "    print(len(sqls))    \n",
    "    with open(f'/opt/hdd/datasets/user/cardbench/{join_type}_workload/{dataset_name}_without_pg_est_card.sql', \"w\") as f:\n",
    "        for sql in sqls:\n",
    "            f.write(sql + \"\\n\")\n",
    "    \n",
    "    return sqls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 37.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:00<00:00, 46.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 40.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 35.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 35.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 47.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 46.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:02<00:00, 23.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 37.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271/271 [00:02<00:00, 93.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:01<00:00, 33.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 30.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:01<00:00, 44.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:01<00:00, 29.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:01<00:00, 37.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:02<00:00, 116.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:01<00:00, 36.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:01<00:00, 33.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 41.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 31.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_names = ['accidents','airline','cms_synthetic_patient_data_omop','consumer','covid19_weathersource_com','crypto_bitcoin_cash','employee','ethereum_blockchain','geo_openstreetmap','github_repos','human_variant_annotation','idc_v10','movielens','open_targets_genetics','samples','stackoverflow','tpch_10G','usfs_fia','uspto_oce_claims','wikipedia']\n",
    "for dataset_name in dataset_names:\n",
    "    get_cardbench_dataset('binary_join', dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waiting for raw data\n",
    "dataset_names = ['accidents','airline','cms_synthetic_patient_data_omop','consumer','covid19_weathersource_com','crypto_bitcoin_cash','employee','ethereum_blockchain','geo_openstreetmap','github_repos','human_variant_annotation','idc_v10','movielens','open_targets_genetics','samples','stackoverflow','tpch_10G','usfs_fia','uspto_oce_claims','wikipedia']\n",
    "for dataset_name in dataset_names:\n",
    "    get_pg_est_card(dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "price",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
