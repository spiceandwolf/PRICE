{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-03 01:35:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--bin_size BIN_SIZE]\n",
      "                             [--table_dim TABLE_DIM] [--filter_dim FILTER_DIM]\n",
      "                             [--query_hidden_dim QUERY_HIDDEN_DIM]\n",
      "                             [--final_hidden_dim FINAL_HIDDEN_DIM]\n",
      "                             [--output_dim OUTPUT_DIM] [--n_embd N_EMBD]\n",
      "                             [--n_layers N_LAYERS] [--n_heads N_HEADS]\n",
      "                             [--dropout_rate DROPOUT_RATE]\n",
      "                             [--batch_size BATCH_SIZE] [--lr LR] [--wd WD]\n",
      "                             [--step_size STEP_SIZE] [--gamma GAMMA]\n",
      "                             [--epochs EPOCHS]\n",
      "                             [--domain_config_path DOMAIN_CONFIG_PATH]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "                             [--reweight_eta REWEIGHT_ETA]\n",
      "                             [--reweight_eps REWEIGHT_EPS]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=/home/user/.local/share/jupyter/runtime/kernel-v2-489222ExhgTBP3nGw8.json could match --filter_dim, --final_hidden_dim\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/price/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from model.encoder import RegressionModel\n",
    "from utils.model.doremi_dataset import load_dataset_features, make_feature_datasets, make_train_feature_dataloaders\n",
    "from utils.model.padding import features_padding\n",
    "from utils.model.qerror import get_qerror\n",
    "from utils.model.args import get_args\n",
    "\n",
    "print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "TRAIN_LIST = ['accidents', 'airline', 'baseball', 'basketball', 'carcinogenesis', 'ccs', 'chembl', 'consumer',\n",
    "              'credit', 'employee', 'financial', 'fnhk', 'grants', 'hepatitis', 'hockey', 'legalacts', 'movielens',\n",
    "              'sakila', 'sap', 'seznam', 'ssb', 'talkingdata', 'telstra', 'tournament', 'tpc_h', 'tubepricing']\n",
    "\n",
    "args = get_args()\n",
    "print(args)\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "train_data, train_labels, train_pg_est_cards, \\\n",
    "train_n_join_cols, train_n_fanouts, train_n_tables, train_n_filter_cols, domain_ids = load_dataset_features(bin_size=args.bin_size, dataset_list=TRAIN_LIST, train_or_test='train', usage='pretrain')\n",
    "\n",
    "max_n_join_col, max_n_fanout, max_n_table, max_n_filter_col = max(train_n_join_cols), max(train_n_fanouts), max(train_n_tables), max(train_n_filter_cols)\n",
    "train_data, train_padding_masks = features_padding(args.bin_size, args.table_dim, args.filter_dim,\n",
    "                                                   train_data, train_n_join_cols, train_n_fanouts, train_n_tables, train_n_filter_cols,\n",
    "                                                   max_n_join_col, max_n_fanout, max_n_table, max_n_filter_col)\n",
    "print(\"dataset padding done!!\")\n",
    "train_dataset = make_feature_datasets(train_data, train_labels, train_pg_est_cards, train_padding_masks,\n",
    "                                      train_n_join_cols, train_n_fanouts, train_n_tables, train_n_filter_cols,\n",
    "                                      train_or_test='train', domain_ids=domain_ids)\n",
    "\n",
    "\n",
    "train_loader = make_train_feature_dataloaders(train_dataset, args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RegressionModel(n_join_col=max_n_join_col, n_fanout=max_n_fanout, n_table=max_n_table, n_filter_col=max_n_filter_col,\n",
    "                        hist_dim=args.bin_size, table_dim=args.table_dim, filter_dim=args.filter_dim,\n",
    "                        query_hidden_dim=args.query_hidden_dim, final_hidden_dim=args.final_hidden_dim, output_dim=args.output_dim,\n",
    "                        n_embd=args.n_embd, n_layers=args.n_layers, n_heads=args.n_heads, dropout_rate=args.dropout_rate).to(device)\n",
    "# model = nn.DataParallel(model, device_ids=[0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "prepare domains reweight\n",
    "difference:\n",
    "args.domain_config_path\n",
    "args.reweight_eta\n",
    "args.reweight_eps\n",
    "args.output_dir\n",
    "'''\n",
    "reference_model_path = f'{current_dir}/results/model_params.pth'\n",
    "print(f\"load model from {reference_model_path}\")\n",
    "reference_model = RegressionModel(n_join_col=max_n_join_col, n_fanout=max_n_fanout, n_table=max_n_table, n_filter_col=max_n_filter_col,\n",
    "                                    hist_dim=args.bin_size, table_dim=args.table_dim, filter_dim=args.filter_dim,\n",
    "                                    query_hidden_dim=args.query_hidden_dim, final_hidden_dim=args.final_hidden_dim, output_dim=args.output_dim,\n",
    "                                    n_embd=args.n_embd, n_layers=args.n_layers, n_heads=args.n_heads, dropout_rate=args.dropout_rate).to(device)\n",
    "reference_model = nn.DataParallel(reference_model)\n",
    "reference_model.load_state_dict(torch.load(reference_model_path))\n",
    "\n",
    "with open(args.domain_config_path, 'r') as f:\n",
    "    domain_config = json.load(f)\n",
    "\n",
    "train_domain_weights_dict = domain_config['train_domain_weights']\n",
    "\n",
    "# whenever we convert dict to array, we sort by key\n",
    "domain_list = list(sorted(train_domain_weights_dict.keys()))\n",
    "\n",
    "sampling_weights = torch.tensor([train_domain_weights_dict[domain] for domain in domain_list])\n",
    "    \n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_domain_weight = sum(train_domain_weights_dict.values())\n",
    "model.register_buffer('train_domain_weights', torch.tensor(\n",
    "        [train_domain_weights_dict[domain] / total_domain_weight for domain in domain_list]))\n",
    "model.register_buffer('avg_domain_weights', model.train_domain_weights.clone())\n",
    "model.register_buffer('perdomain_scores', torch.ones(len(train_domain_weights_dict)) * np.log(len(domain_list)))\n",
    "model.register_buffer('update_counter', torch.tensor(1))\n",
    "\n",
    "'''\n",
    "END\n",
    "'''\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    print('--'*30)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    all_output, all_label = [], []\n",
    "    # pertoken_scores, token_masks, domain_ids = [], [], []\n",
    "    for i, (feature, label, pg_est_card, padding_mask, n_join_col, n_fanout, n_table, n_filter_col, domain_ids) in enumerate(train_loader):\n",
    "        feature = feature.to(torch.float).to(device)\n",
    "        padding_mask = padding_mask.to(torch.float).to(device)\n",
    "        n_join_col = n_join_col.to(torch.float).to(device).view(-1, 1)\n",
    "        n_fanout = n_fanout.to(torch.float).to(device).view(-1, 1)\n",
    "        n_table = n_table.to(torch.float).to(device).view(-1, 1)\n",
    "        n_filter_col = n_filter_col.to(torch.float).to(device).view(-1, 1)\n",
    "        pg_est_card = pg_est_card.to(torch.float).to(device).view(-1, 1)\n",
    "        pg_est_card = torch.log(pg_est_card + 1) + 1\n",
    "        label = torch.log(label.to(torch.float).to(device) + 1) + 1\n",
    "        label = label.view(1, -1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(feature, pg_est_card, padding_mask, n_join_col, n_fanout, n_table, n_filter_col).view(1, -1)\n",
    "        pertoken_loss = criterion(output, label)\n",
    "        '''\n",
    "        TODO:\n",
    "        '''\n",
    "        reference_model.train()\n",
    "        with torch.no_grad():\n",
    "            reference_output = reference_model(feature, pg_est_card, padding_mask, n_join_col, n_fanout, n_table, n_filter_col).view(1, -1)\n",
    "        reference_pertoken_loss = criterion(reference_output, label)\n",
    "        # token_masks = outputs.token_mask\n",
    "        excess_loss = pertoken_loss - reference_pertoken_loss\n",
    "        \n",
    "        scores = excess_loss.detach()\n",
    "        # token_masks = token_masks.detach()\n",
    "        \n",
    "        # update domain weights\n",
    "        wandb_log_dict = {}\n",
    "        train_domain_weights = model.train_domain_weights.clone()\n",
    "\n",
    "        perdomain_scores = []\n",
    "        for domain_id in range(len(train_domain_weights)):\n",
    "            domain_mask = (domain_ids == domain_id)\n",
    "            # perdomain_scores_mask = token_masks[domain_mask]\n",
    "            if domain_mask.sum() > 0:\n",
    "                # curr_domain_scores = torch.clip(scores[domain_mask][perdomain_scores_mask], min=0).mean()\n",
    "                curr_domain_scores = torch.clip(scores[domain_mask], min=0).mean()\n",
    "            else:\n",
    "                curr_domain_scores = model.perdomain_scores[domain_id]\n",
    "            perdomain_scores.append(curr_domain_scores)\n",
    "        model.perdomain_scores[:] = torch.tensor(perdomain_scores).float()\n",
    "        log_new_train_domain_weights = torch.log(train_domain_weights) + args.reweight_eta * model.perdomain_scores\n",
    "        log_new_train_domain_weights = log_new_train_domain_weights - torch.logsumexp(log_new_train_domain_weights, dim=0)\n",
    "        train_domain_weights = (1-args.reweight_eps) * torch.exp(log_new_train_domain_weights) + args.reweight_eps / len(log_new_train_domain_weights)\n",
    "        model.update_counter += 1\n",
    "        model.train_domain_weights[:] = train_domain_weights\n",
    "        model.avg_domain_weights[:] = (model.avg_domain_weights * (model.update_counter - 1) + train_domain_weights) / model.update_counter\n",
    "\n",
    "        # for domain_idx in range(len(train_domain_weights)):\n",
    "        #     domain_name = domain_list[domain_idx]\n",
    "        #     wandb_log_dict[f'avg_domain_weights/{domain_name}'] = model.avg_domain_weights[domain_idx].item()\n",
    "        #     wandb_log_dict[f'train_domain_weights/{domain_name}'] = model.train_domain_weights[domain_idx].item()\n",
    "        #     wandb_log_dict[f'perdomain_scores/{domain_name}'] = model.perdomain_scores[domain_idx].item()\n",
    "        # wandb_log_dict['max_domain_id'] = domain_ids.max().item()\n",
    "        # wandb.log(wandb_log_dict, commit=False)\n",
    "\n",
    "        # if doing non-uniform sampling, normalize by inverse sampling weight\n",
    "        train_domain_weights = train_domain_weights / sampling_weights.to(train_domain_weights.device)\n",
    "        train_domain_weights = train_domain_weights / train_domain_weights.sum()\n",
    "        curr_domain_weights = train_domain_weights[domain_ids].unsqueeze(-1).expand_as(pertoken_loss).detach()\n",
    "\n",
    "        # curr_domain_weights = curr_domain_weights * token_mask\n",
    "\n",
    "        # renormalize\n",
    "        normalizer = curr_domain_weights.detach().sum()\n",
    "        normalizer = torch.clip(normalizer, min=1e-10)\n",
    "\n",
    "        # token_mask = token_mask.detach().type(pertoken_loss.dtype)\n",
    "        loss = (pertoken_loss * curr_domain_weights.detach()).sum() / normalizer\n",
    "        \n",
    "        '''\n",
    "        END\n",
    "        '''\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * len(feature)\n",
    "        all_output.extend(output.cpu().data.tolist()[0])\n",
    "        all_label.extend(label.cpu().data.tolist()[0])\n",
    "\n",
    "    scheduler.step()\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f\"epoch: {epoch}, train loss: {train_loss}\")\n",
    "\n",
    "    all_output, all_label = np.array(all_output), np.array(all_label)\n",
    "    q_error = get_qerror(all_output, all_label, cuda=False, do_scale=True, percentile_list=[30, 50, 80, 90, 95, 99])\n",
    "    print('train q-error: 30%:', q_error[0], '  50%:', q_error[1], '  80%:', q_error[2], '  90%:', q_error[3], '  95%:', q_error[4], '  99%:', q_error[5])\n",
    "\n",
    "print('done!')\n",
    "torch.save(model.state_dict(), f'results/doremi_pretrain_params.pth')\n",
    "print('model saved in results/pretrain_params.pth')\n",
    "print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "'''\n",
    "avg_domain_weights_dict = {}\n",
    "for i in range(len(model.avg_domain_weights)):\n",
    "    domain_name = domain_list[i]\n",
    "    avg_domain_weights_dict[domain_name] = model.avg_domain_weights[i].item()\n",
    "\n",
    "# save avg domain weights to json\n",
    "avg_domain_weights_file = Path(args.output_dir) / 'avg_domain_weights.json'\n",
    "with open(avg_domain_weights_file, 'w') as f:\n",
    "    json.dump(avg_domain_weights_dict, f, indent=2)\n",
    "\n",
    "# also save to configs dir\n",
    "config_dict = {\"train_domain_weights\": avg_domain_weights_dict,\n",
    "                \"eval_domain_weights\": avg_domain_weights_dict}\n",
    "config_dict_file = Path(__file__).parent.parent / 'configs' / f\"{Path(args.output_dir).name}.json\"\n",
    "with open(config_dict_file, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "'''\n",
    "END\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "price",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
